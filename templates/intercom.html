{% extends "base.html" %}

{% block title %}Intercom - Birdbox{% endblock %}

{% block head %}
<style>
    /* Full viewport layout - using svh (small viewport height) to account for mobile browser UI */
    .intercom-container {
        height: 100vh;
        /* fallback for older browsers */
        height: 100svh;
        /* small viewport height - excludes browser UI */
        display: flex;
        flex-direction: column;
        margin: 0;
        padding: 0;
    }

    /* Video container fills remaining space */
    .video-container {
        flex-grow: 1;
        background-color: #000;
        display: flex;
        align-items: center;
        justify-content: center;
        overflow: hidden;
        position: relative;
    }

    /* Video maintains aspect ratio while filling container */
    #videoFeed {
        width: 100%;
        height: 100%;
        object-fit: contain;
    }

    /* Status text in button area */
    .status-text {
        font-size: 0.8rem;
        color: rgba(255, 255, 255, 0.7);
        white-space: nowrap;
        margin-bottom: 12px;
        text-align: center;
    }

    /* Audio prompt overlay at top center */
    .audio-prompt {
        position: absolute;
        top: 16px;
        left: 50%;
        transform: translateX(-50%);
        background-color: rgba(0, 0, 0, 0.85);
        color: #fff;
        padding: 12px 24px;
        border-radius: 8px;
        font-size: 0.95rem;
        font-weight: 500;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.5);
        z-index: 20;
        display: none;
        animation: fadeIn 0.3s ease-in;
        cursor: pointer;
    }

    .audio-prompt.visible {
        display: block;
    }

    @keyframes fadeIn {
        from {
            opacity: 0;
            transform: translateX(-50%) translateY(-10px);
        }

        to {
            opacity: 1;
            transform: translateX(-50%) translateY(0);
        }
    }

    /* Button container below video - centered vertically */
    .button-container {
        background-color: #000;
        padding: 20px;
        padding-bottom: max(20px, env(safe-area-inset-bottom));
        /* respect safe area on iOS */
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        min-height: 25vh;
    }

    /* Button wrapper for max-width constraint */
    .button-wrapper {
        display: flex;
        flex-direction: row;
        gap: 12px;
        width: 100%;
        max-width: 600px;
    }

    /* Buttons in horizontal row, equal width */
    .button-container .btn {
        flex: 1;
        padding: 14px 20px;
        font-size: 1rem;
        font-weight: 500;
        position: relative;
    }

    /* Button status indicators */
    .btn-status-icon {
        display: inline-block;
        margin-left: 8px;
        width: 16px;
        height: 16px;
        vertical-align: middle;
    }

    .spinner-border-sm-custom {
        width: 16px;
        height: 16px;
        border-width: 2px;
    }

    /* Responsive adjustments for smaller screens */
    @media (max-width: 576px) {
        .button-container {
            padding: 16px;
            padding-bottom: max(16px, env(safe-area-inset-bottom));
            min-height: 20vh;
        }

        .button-wrapper {
            gap: 10px;
        }

        .button-container .btn {
            padding: 12px 16px;
            font-size: 0.95rem;
        }

        .status-text {
            font-size: 0.75rem;
            margin-bottom: 10px;
        }

        .audio-prompt {
            top: 12px;
            padding: 10px 20px;
            font-size: 0.9rem;
        }
    }

    /* Hide base template container padding for full-screen layout */
    body.p-4 {
        padding: 0 !important;
    }

    body .container {
        padding: 0 !important;
        margin: 0 !important;
        max-width: 100% !important;
    }
</style>
{% endblock %}

{% block body %}
<div class="intercom-container">
    <!-- Video fills remaining space -->
    <div class="video-container">
        <video id="videoFeed" autoplay playsinline muted></video>
        <div id="audioPrompt" class="audio-prompt">üëÜ Touch to start audio</div>
    </div>

    <!-- Button container below video -->
    <div class="button-container">
        <div id="connectionStatus" class="status-text">Connecting...</div>
        <div class="button-wrapper">
            <button id="openGatesBtn" class="btn btn-warning" hx-post="/api/open-gates" hx-swap="none">
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                    class="bi bi-door-open-fill" viewBox="0 0 16 16">
                    <path
                        d="M1.5 15a.5.5 0 0 0 0 1h13a.5.5 0 0 0 0-1H13V2.5A1.5 1.5 0 0 0 11.5 1H11V.5a.5.5 0 0 0-.57-.495l-7 1A.5.5 0 0 0 3 1.5V15zM11 2h.5a.5.5 0 0 1 .5.5V15h-1zm-2.5 8c-.276 0-.5-.448-.5-1s.224-1 .5-1 .5.448.5 1-.224 1-.5 1" />
                </svg>
                <span id="openGatesText">Open Gates</span>
                <span id="gatesStatusIcon" class="btn-status-icon"></span>
            </button>

            <button id="transmitBtn" class="btn btn-secondary" disabled>
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                    class="bi bi-mic-fill" viewBox="0 0 16 16">
                    <path d="M5 3a3 3 0 0 1 6 0v5a3 3 0 0 1-6 0z" />
                    <path
                        d="M3.5 6.5A.5.5 0 0 1 4 7v1a4 4 0 0 0 8 0V7a.5.5 0 0 1 1 0v1a5 5 0 0 1-4.5 4.975V15h3a.5.5 0 0 1 0 1h-7a.5.5 0 0 1 0-1h3v-2.025A5 5 0 0 1 3 8V7a.5.5 0 0 1 .5-.5" />
                </svg>
                <span id="transmitText">Transmit</span>
            </button>
        </div>
    </div>
</div>

<!-- Audio element for doorbell audio -->
<audio id="audio" autoplay playsinline></audio>
{% endblock %}

{% block scripts %}
<script>
    const log = (...args) => {
        const timestamp = new Date().toISOString();
        console.log(`[${timestamp}] [webrtc]`, ...args);
    };
    const connectionStatusEl = document.getElementById('connectionStatus');
    const audioPromptEl = document.getElementById('audioPrompt');
    const audioEl = document.getElementById('audio');
    const transmitBtn = document.getElementById('transmitBtn');
    const transmitText = document.getElementById('transmitText');
    const openGatesBtn = document.getElementById('openGatesBtn');
    const gatesStatusIcon = document.getElementById('gatesStatusIcon');

    // Detect PWA mode
    const isPWA = window.matchMedia('(display-mode: standalone)').matches ||
        window.matchMedia('(display-mode: fullscreen)').matches ||
        window.navigator.standalone === true ||
        document.referrer.includes('android-app://');

    log('Running in PWA mode:', isPWA);
    if (!isPWA) {
        log('‚ö†Ô∏è  Not running as installed PWA - address bar will be visible');
        log('Install from Share menu ‚Üí "Add to Home Screen" for full-screen experience');
    } else {
        log('‚úÖ Running as installed PWA - full-screen mode');
    }

    // Check secure context for microphone access
    if (!window.isSecureContext) {
        console.warn('‚ö†Ô∏è  Not running in secure context (HTTPS/localhost)');
        console.warn('‚ö†Ô∏è  Microphone access will be blocked');
        console.warn(`   Current: ${window.location.protocol}//${window.location.hostname}`);
        console.warn('   Solution: Use HTTPS or localhost for microphone features');
    } else {
        log('‚úÖ Secure context - microphone access available');
    }

    let pc = null;
    let socket = null;
    let localStream = null;
    let isTransmitting = false;
    let othersTransmitting = false;

    // Add this function to debug media devices
    async function debugMediaDevices() {
        try {
            const devices = await navigator.mediaDevices.enumerateDevices();
            log('Available media devices:', devices);

            const audioInputs = devices.filter(device => device.kind === 'audioinput');
            log('Audio input devices:', audioInputs);

            if (audioInputs.length === 0) {
                console.warn('‚ö†Ô∏è  No audio input devices found!');
            }
        } catch (e) {
            console.error('Failed to enumerate devices:', e);
        }
    }

    function setConnectionStatus(text) {
        connectionStatusEl.textContent = text;
    }

    function setGatesButtonStatus(status) {
        // status can be: 'loading', 'success', 'error', or null (clear)
        gatesStatusIcon.innerHTML = '';

        if (status === 'loading') {
            // Show spinner
            gatesStatusIcon.innerHTML = '<span class="spinner-border spinner-border-sm-custom" role="status" aria-hidden="true"></span>';
            openGatesBtn.disabled = true;
        } else if (status === 'success') {
            // Show checkmark
            gatesStatusIcon.innerHTML = `<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-check-circle-fill" viewBox="0 0 16 16">
                <path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0m-3.97-3.03a.75.75 0 0 0-1.08.022L7.477 9.417 5.384 7.323a.75.75 0 0 0-1.06 1.06L6.97 11.03a.75.75 0 0 0 1.079-.02l3.992-4.99a.75.75 0 0 0-.01-1.05z"/>
            </svg>`;
            openGatesBtn.disabled = true;
            // Clear after 2 seconds
            setTimeout(() => {
                setGatesButtonStatus(null);
            }, 2000);
        } else if (status === 'error') {
            // Show X
            gatesStatusIcon.innerHTML = `<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-x-circle-fill" viewBox="0 0 16 16">
                <path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0M5.354 4.646a.5.5 0 1 0-.708.708L7.293 8l-2.647 2.646a.5.5 0 0 0 .708.708L8 8.707l2.646 2.647a.5.5 0 0 0 .708-.708L8.707 8l2.647-2.646a.5.5 0 0 0-.708-.708L8 7.293z"/>
            </svg>`;
            openGatesBtn.disabled = true;
            // Clear after 2 seconds
            setTimeout(() => {
                setGatesButtonStatus(null);
            }, 2000);
        } else {
            // Clear/null - re-enable button
            openGatesBtn.disabled = false;
        }
    }

    function updateTransmitButton() {
        if (!pc || pc.connectionState !== 'connected') {
            transmitBtn.disabled = true;
            transmitBtn.className = 'btn btn-secondary';
            transmitText.textContent = 'Transmit';
            return;
        }

        if (othersTransmitting) {
            transmitBtn.disabled = true;
            transmitBtn.className = 'btn btn-warning';
            transmitText.textContent = 'Line Busy';
        } else if (isTransmitting) {
            transmitBtn.disabled = false;
            transmitBtn.className = 'btn btn-danger';
            transmitText.textContent = 'Stop';
        } else {
            transmitBtn.disabled = false;
            transmitBtn.className = 'btn btn-success';
            transmitText.textContent = 'Transmit';
        }
    }

    async function startTransmitting() {
        if (isTransmitting || othersTransmitting) return;

        // Check if we're in a secure context (HTTPS or localhost)
        if (!window.isSecureContext) {
            const protocol = window.location.protocol;
            const hostname = window.location.hostname;
            console.error('Microphone access denied: not in secure context');
            alert(
                'üîí Microphone Access Blocked\n\n' +
                'Microphone access requires HTTPS or localhost.\n\n' +
                `Current: ${protocol}//${hostname}\n\n` +
                'To fix:\n' +
                '‚Ä¢ Access via https:// instead of http://\n' +
                '‚Ä¢ Or use localhost/127.0.0.1 for testing\n' +
                '‚Ä¢ Configure a reverse proxy with SSL certificate'
            );
            return;
        }

        log('Starting transmission...');

        // Optimistically set transmitting state
        isTransmitting = true;
        updateTransmitButton();

        try {
            // Request microphone access if we don't have it
            if (!localStream) {
                log('Requesting microphone access...');

                // Try with more permissive constraints first
                try {
                    localStream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: false,  // Try disabling first
                            noiseSuppression: false,
                            autoGainControl: false,
                        }
                    });
                } catch (e) {
                    // Fallback to even more basic constraints
                    console.warn('First attempt failed, trying basic constraints:', e);
                    localStream = await navigator.mediaDevices.getUserMedia({
                        audio: true  // Most permissive
                    });
                }
                log('Microphone access granted');
            }

            // Wait for peer connection to be in a stable state
            if (pc.connectionState !== 'connected' && pc.iceConnectionState !== 'connected') {
                log('Waiting for peer connection to be ready...');
                // Wait a bit for connection to stabilize
                await new Promise(resolve => setTimeout(resolve, 500));
            }

            // Add microphone track to peer connection
            const audioTrack = localStream.getAudioTracks()[0];

            // Check if we already have a sender for our local audio
            let sender = pc.getSenders().find(s => s.track && s.track.kind === 'audio' && s.track.id === audioTrack.id);

            if (!sender) {
                // Add the track - this triggers renegotiation
                sender = pc.addTrack(audioTrack, localStream);
                log('Added microphone track to peer connection');

                // Wait a moment before creating offer
                await new Promise(resolve => setTimeout(resolve, 100));

                // Create and send new offer to trigger renegotiation
                const offer = await pc.createOffer();
                await pc.setLocalDescription(offer);
                socket.send(JSON.stringify({ type: 'offer', sdp: offer.sdp }));
                log('Sent renegotiation offer with microphone track');
            } else {
                log('Microphone track already added');
            }

            // Request transmission from server
            socket.send(JSON.stringify({ type: 'start_ptt' }));

        } catch (e) {
            console.error('Failed to start transmission:', e);
            alert('Failed to access microphone: ' + e.message);
            // Reset state on error
            isTransmitting = false;
            updateTransmitButton();
        }
    }

    async function stopTransmitting() {
        if (!isTransmitting) return;

        log('Stopping transmission...');

        // Mark as inactive
        isTransmitting = false;
        // Clear others transmitting flag when we stop
        othersTransmitting = false;
        updateTransmitButton();

        // Remove microphone track from peer connection
        if (localStream) {
            const audioTrack = localStream.getAudioTracks()[0];
            const sender = pc.getSenders().find(s => s.track && s.track.id === audioTrack.id);
            if (sender) {
                pc.removeTrack(sender);
                log('Removed microphone track from peer connection');

                // Create and send new offer to trigger renegotiation
                const offer = await pc.createOffer();
                await pc.setLocalDescription(offer);
                socket.send(JSON.stringify({ type: 'offer', sdp: offer.sdp }));
                log('Sent renegotiation offer without microphone track');
            }
        }

        // Notify server
        socket.send(JSON.stringify({ type: 'stop_ptt' }));
    }

    function toggleTransmit() {
        if (isTransmitting) {
            stopTransmitting();
        } else {
            startTransmitting();
        }
    }

    // Transmit button event handler - simple toggle on click
    transmitBtn.addEventListener('click', toggleTransmit);

    async function connect() {
        setConnectionStatus('Connecting...');
        // Client-server architecture: no STUN needed
        // Server advertises its IP(s) as host candidates via NAT 1:1 mapping
        // In dual-network setups, server advertises both LAN and public IPs
        // WebRTC ICE will automatically select the best route:
        // - LAN clients connect directly via LAN IP (fast, no router)
        // - External clients connect via public IP (NAT forwarding)
        pc = new RTCPeerConnection({
            iceServers: []  // Empty - rely on server's advertised candidates
        });

        // Ensure the offer contains media m-lines for receiving server audio and video
        try {
            pc.addTransceiver('audio', { direction: 'recvonly' });
            pc.addTransceiver('video', { direction: 'recvonly' });
        } catch (e) {
            log('addTransceiver failed (older browser?)', e);
        }

        pc.onicecandidate = (e) => {
            if (e.candidate) {
                // Filter out mDNS candidates from browser console logging (they're ignored by server anyway)
                if (!e.candidate.address || !e.candidate.address.includes('.local')) {
                    log('local ICE candidate:', {
                        candidate: e.candidate.candidate,
                        sdpMid: e.candidate.sdpMid,
                        sdpMLineIndex: e.candidate.sdpMLineIndex,
                        type: e.candidate.type,
                        protocol: e.candidate.protocol,
                        address: e.candidate.address,
                        port: e.candidate.port
                    });
                } else {
                    // Log mDNS candidates at debug level (only visible with console.debug enabled)
                    console.debug('[webrtc] local ICE candidate (mDNS):', {
                        candidate: e.candidate.candidate,
                        address: e.candidate.address
                    });
                }
                socket.send(JSON.stringify({
                    type: 'candidate',
                    candidate: e.candidate.candidate,
                    sdpMid: e.candidate.sdpMid,
                    sdpMLineIndex: e.candidate.sdpMLineIndex
                }));
            } else {
                log('‚úì ICE candidate gathering complete');
            }
        };
        pc.onicegatheringstatechange = () => {
            log('ICE gathering state changed:', pc.iceGatheringState);
        };
        pc.onsignalingstatechange = () => {
            log('signaling state changed:', pc.signalingState);
        };
        pc.oniceconnectionstatechange = () => {
            log('ICE connection state changed:', pc.iceConnectionState);
            switch (pc.iceConnectionState) {
                case 'connected':
                    setConnectionStatus('Connected');
                    log('‚úì ICE connection established');
                    break;
                case 'disconnected':
                    setConnectionStatus('Disconnected');
                    log('‚ö† ICE connection disconnected');
                    break;
                case 'failed':
                    setConnectionStatus('Failed');
                    log('‚úó ICE connection failed');
                    break;
                case 'checking':
                    setConnectionStatus('Checking...');
                    log('‚ãØ ICE checking connectivity...');
                    break;
                case 'new':
                    log('ICE state: new');
                    break;
                case 'completed':
                    log('‚úì ICE connection completed');
                    break;
                case 'closed':
                    setConnectionStatus('Closed');
                    log('ICE connection closed');
                    break;
            }
            updateTransmitButton();
        };
        pc.onconnectionstatechange = () => {
            log('peer connection state changed:', pc.connectionState);
            updateTransmitButton();
        };
        pc.ontrack = (e) => {
            log('remote track received', e.track.kind, e.streams[0]);
            if (e.track.kind === 'audio') {
                audioEl.srcObject = e.streams[0];
                // Explicitly try to play audio (handle autoplay policy)
                audioEl.play().then(() => {
                    log('‚úì Audio autoplay succeeded');
                }).catch(err => {
                    log('Audio autoplay blocked, showing prompt:', err.message);
                    // Show the "Touch to start audio" message
                    audioPromptEl.classList.add('visible');

                    // Set up one-time click handler to enable audio
                    const enableAudio = () => {
                        audioEl.play().then(() => {
                            log('‚úì Audio playback started after user interaction');
                            audioPromptEl.classList.remove('visible');
                        }).catch(e => {
                            log('Audio play failed:', e);
                        });
                    };

                    // Listen for any click on the document
                    document.addEventListener('click', enableAudio, { once: true });
                    // Also make the prompt itself clickable
                    audioPromptEl.addEventListener('click', enableAudio, { once: true });
                });
            } else if (e.track.kind === 'video') {
                document.getElementById('videoFeed').srcObject = e.streams[0];
            }
        };

        // Use wss:// for HTTPS, ws:// for HTTP
        const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
        socket = new WebSocket(`${wsProtocol}//${location.host}/ws`);
        socket.onopen = async () => {
            log('‚úì WebSocket connected');
            const offer = await pc.createOffer();
            log('created offer, setting local description...');
            await pc.setLocalDescription(offer);
            log('sending offer to server');
            socket.send(JSON.stringify({ type: 'offer', sdp: offer.sdp }));
        };
        socket.onmessage = async (ev) => {
            const msg = JSON.parse(ev.data);
            if (msg.type === 'answer') {
                log('‚úì received answer from server');
                await pc.setRemoteDescription({ type: 'answer', sdp: msg.sdp });
                log('remote description set');
            } else if (msg.type === 'candidate') {
                log('remote ICE candidate:', {
                    candidate: msg.candidate,
                    sdpMid: msg.sdpMid,
                    sdpMLineIndex: msg.sdpMLineIndex
                });
                try {
                    await pc.addIceCandidate({
                        candidate: msg.candidate,
                        sdpMid: msg.sdpMid ?? null,
                        sdpMLineIndex: msg.sdpMLineIndex ?? null,
                    });
                    log('‚úì remote candidate added');
                } catch (e) {
                    console.error('‚úó addIceCandidate failed:', e);
                }
            } else if (msg.type === 'ptt_granted') {
                log('‚úì Transmission granted');
                // Server confirmed transmission lock acquired
            } else if (msg.type === 'ptt_denied') {
                log('‚úó Transmission denied:', msg.reason);
                // Server denied - line is busy, revert state
                isTransmitting = false;
                updateTransmitButton();

                // Revert by removing the track
                if (localStream) {
                    const audioTrack = localStream.getAudioTracks()[0];
                    const sender = pc.getSenders().find(s => s.track && s.track.id === audioTrack.id);
                    if (sender) {
                        pc.removeTrack(sender);
                    }
                }
            } else if (msg.type === 'ptt_state') {
                log('Transmission state update:', msg.transmitting);
                // If someone started transmitting and it's not us, set othersTransmitting
                // If someone stopped transmitting, always clear othersTransmitting
                if (msg.transmitting && !isTransmitting) {
                    othersTransmitting = true;
                } else if (!msg.transmitting) {
                    othersTransmitting = false;
                }
                updateTransmitButton();
            }
        };
        socket.onclose = () => {
            log('‚úó WebSocket closed');
            setConnectionStatus('Disconnected');
        };
        socket.onerror = (e) => {
            console.error('‚úó WebSocket error:', e);
            setConnectionStatus('Error');
        };
    }

    // Auto-connect on page load
    document.addEventListener('DOMContentLoaded', () => {
        debugMediaDevices();
        log('Auto-connecting to intercom...');
        connect();
    });

    // Handle gates button with HTMX events
    openGatesBtn.addEventListener('htmx:beforeRequest', function (evt) {
        setGatesButtonStatus('loading');
    });

    openGatesBtn.addEventListener('htmx:afterRequest', function (evt) {
        if (evt.detail.successful) {
            // Success response
            setGatesButtonStatus('success');
        } else {
            // Error response
            setGatesButtonStatus('error');
        }
    });
</script>
{% endblock %}